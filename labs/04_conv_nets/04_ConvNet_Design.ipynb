{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet design\n",
    "\n",
    "In this notebook we will design our own ConvNet and see some existing applications.\n",
    "\n",
    "We will also see the three different methods to define a Keras model:\n",
    "\n",
    "- Sequential\n",
    "- Functional\n",
    "- Object-Oriented\n",
    "\n",
    "The goal of this notebook is not to compare models performance, as we are limited on compute power, but to compare model architectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step: \n",
      "11501568/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(((50000, 28, 28, 1), (50000,)),\n",
       " ((10000, 28, 28, 1), (10000,)),\n",
       " ((10000, 28, 28, 1), (10000,)))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "h, w = x_train.shape[1:]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], h, w, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], h, w, 1)\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=10000, random_state=42)\n",
    "\n",
    "(x_train.shape, y_train.shape), (x_val.shape, y_val.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].squeeze(-1))\n",
    "plt.title(y_train[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] unique labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"{} unique labels.\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeNet\n",
    "\n",
    "Let's define a (slightly modified) LeNet model introduced by Yann Le Cun in 1998 ([paper url](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). The model is very simple and can be defined with the **Sequential** API.\n",
    "\n",
    "![lenet archi](images/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNet-5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "C1 (Conv2D)                  (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "S2 (MaxPooling2D)            (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "C3 (Conv2D)                  (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "S4 (AveragePooling2D)        (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "C5 (Conv2D)                  (None, 1, 1, 120)         48120     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "F6 (Dense)                   (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, AvgPool2D\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "lenet = Sequential(name=\"LeNet-5\")\n",
    "\n",
    "lenet.add(Conv2D(6, kernel_size=(5, 5), activation=\"tanh\", padding=\"same\", input_shape=input_shape, name=\"C1\"))\n",
    "lenet.add(MaxPool2D(pool_size=(2, 2), name=\"S2\"))\n",
    "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', name=\"C3\"))\n",
    "lenet.add(AvgPool2D(pool_size=(2, 2), name=\"S4\"))\n",
    "lenet.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', name=\"C5\"))\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(84, activation='tanh', name=\"F6\"))\n",
    "lenet.add(Dense(10, activation='softmax'))\n",
    "\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hugo\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 10s 48ms/step - loss: 0.5974 - accuracy: 0.8415 - val_loss: 0.2565 - val_accuracy: 0.9241\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 9s 47ms/step - loss: 0.2153 - accuracy: 0.9370 - val_loss: 0.1813 - val_accuracy: 0.9438\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 10s 49ms/step - loss: 0.1539 - accuracy: 0.9539 - val_loss: 0.1243 - val_accuracy: 0.9665\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 10s 49ms/step - loss: 0.1209 - accuracy: 0.9642 - val_loss: 0.1147 - val_accuracy: 0.9659\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 9s 47ms/step - loss: 0.1019 - accuracy: 0.9695 - val_loss: 0.0887 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14715ed96d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "lenet.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08096437901258469, 0.9746999740600586]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while LeNet was first defined using either `tanh` or `sigmoid`, those activations are now rarely used. As seen in Lab02, both activations saturate on very small and large values making their gradient almost null.\n",
    "\n",
    "Now most network use `ReLU` as hidden activation function or one of its derivative (https://keras.io/layers/advanced-activations/).\n",
    "\n",
    "## 2. Inception\n",
    "\n",
    "Inception models were introduced in 2014 by Szegedy et al. ([paper url](https://arxiv.org/abs/1409.4842)).\n",
    "\n",
    "Convolutions have an effective receptive field: the bigger the kernels, and the deeper the model, a features pixel will *see* more image pixels. Read this for a good explanation: [medium blog](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n",
    "\n",
    "In Inception, convolution kernels of different sizes are combined. Small kernels see small clusters of features (think a detail as an eye) while big kernels see big clusters of features (think a face).\n",
    "\n",
    "![inception archi](images/inception.png)\n",
    "\n",
    "This time, use the **Functional** API to define a single Inception layer like the previous image\n",
    "Exemple usage:\n",
    "\n",
    "```python\n",
    "a = Input(shape=(32,))\n",
    "b = Dense(32)(a)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "```\n",
    "\n",
    "The layer is first instancied (first pair of parenthesis) then called on a tensor (second set of parenthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Inputs to a layer should be tensors. Got: None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-05b34c41eab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m         training=training_mode):\n\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m       \u001b[0minput_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;31m# have a `shape` attribute.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Inputs to a layer should be tensors. Got: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: None"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def inception_layer(tensor, n_filters):\n",
    "    # TODO: define the 4 branches\n",
    "    branch1x1 = None\n",
    "    branch5x5 = None\n",
    "    branch3x3 = None\n",
    "    branch_pool = None\n",
    "\n",
    "    # TODO: merge it using Concatenate layer, use Concatenate? for more info\n",
    "    output = None\n",
    "    return output\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=input_shape)\n",
    "x = Conv2D(16, kernel_size=(5, 5), padding=\"same\")(input_tensor)\n",
    "x = inception_layer(x, 32)\n",
    "x = Flatten()(x)\n",
    "output_tensor = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mini_inception = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "mini_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 16)   416         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   544         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 32)   12832       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 32)   4640        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 28, 28, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 28, 28, 112)  0           conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 87808)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           878090      flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 896,522\n",
      "Trainable params: 896,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/mini_inception.py\n",
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def inception_layer(tensor, n_filters):\n",
    "    branch1x1 = Conv2D(n_filters, kernel_size=(1, 1), activation=\"relu\", padding=\"same\")(tensor)\n",
    "    branch5x5 = Conv2D(n_filters, kernel_size=(5, 5), activation=\"relu\", padding=\"same\")(tensor)\n",
    "    branch3x3 = Conv2D(n_filters, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(tensor)\n",
    "\n",
    "    branch_pool = MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(tensor)\n",
    "\n",
    "    output = Concatenate(axis=-1)(\n",
    "        [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=input_shape)\n",
    "x = Conv2D(16, kernel_size=(5, 5), padding=\"same\")(input_tensor)\n",
    "x = inception_layer(x, 32)\n",
    "x = Flatten()(x)\n",
    "output_tensor = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mini_inception = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "mini_inception.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet\n",
    "\n",
    "ResNet (*Residual Networks*) models were introduced by He et al. in 2015 ([paper url](https://arxiv.org/abs/1512.03385)). They found that more layers improved the performance but unfortunatly it was hard to backpropagate the gradients up to first layers.\n",
    "\n",
    "A trick to let the gradients \"*flow*\" easily is to use shortcut connection that let the forward tensor untouched (aka a *residual*):\n",
    "\n",
    "![resnet archi](images/resnet.png)\n",
    "\n",
    "This time, code a ResNet layer using the **Oriented-Object** API:\n",
    "\n",
    "Exemple usage:\n",
    "```python\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Add\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(name=\"ResidualBlock\")\n",
    "        \n",
    "        # TODO: define needed layers, use Add to combine the shortcut with the convs' output.\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        return 42\n",
    "    \n",
    "\n",
    "class MiniResNet(Model):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(n_filters, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.block = ResidualBlock(n_filters)\n",
    "        self.flatten = Flatten()\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # TODO\n",
    "        return 1337\n",
    "\n",
    "\n",
    "mini_resnet = MiniResNet(32)\n",
    "mini_resnet.build((None, *input_shape))\n",
    "mini_resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mini_res_net_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           multiple                  832       \n",
      "_________________________________________________________________\n",
      "ResidualBlock (ResidualBlock multiple                  18496     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  250890    \n",
      "=================================================================\n",
      "Total params: 270,218\n",
      "Trainable params: 270,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/mini_resnet.py\n",
    "from tensorflow.keras.layers import Add, Layer, Activation\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(name=\"ResidualBlock\")\n",
    "\n",
    "        self.conv1 = Conv2D(n_filters, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")\n",
    "        self.conv2 = Conv2D(n_filters, kernel_size=(3, 3), padding=\"same\")\n",
    "        self.add = Add()\n",
    "        self.last_relu = Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(inputs)\n",
    "\n",
    "        y = self.add([x, inputs])\n",
    "        y = self.last_relu(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class MiniResNet(Model):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(n_filters, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.block = ResidualBlock(n_filters)\n",
    "        self.flatten = Flatten()\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.block(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.classifier(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "mini_resnet = MiniResNet(32)\n",
    "mini_resnet.build((None, *input_shape))\n",
    "mini_resnet.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization\n",
    "\n",
    "Batch Normalization is not an architecture but a layer. Introduced by Ioffe et al. in 2015 ([paper url](https://arxiv.org/abs/1502.03167)). Here is an extract from their abstract:\n",
    "\n",
    "> Training Deep Neural Networks is complicated by the fact that the **distribution of each layer’s inputs changes during training, as the parameters of the previous layers change**. This slows down the training by requiring lower learningrates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.  We refer to this phenomenon as **internal covariate shift**, and address the problem by **normalizing layer inputs**.\n",
    "\n",
    "The results are that ConvNet trained with BatchNorm converge faster and with better results. Nowadays all (or almost all) networks are use it or one of its variants. See this [article on normalization](https://arthurdouillard.com/post/normalization/) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classic block is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(Layer):\n",
    "    def __init__(n_filters, kernel_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = Conv2D(n_filters, kernel_size=kernel_size, use_bias=False)\n",
    "        self.bn = BatchNormalization(axis=3)\n",
    "        self.activation = Activation(\"relu\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activation(\n",
    "            self.bn(self.conv(inputs))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That you can place multiple times into your network as Lego blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Separable Convolutions\n",
    "\n",
    "ConvNet usually have a lot of parameters because of their large depth. A trick to trim the number of parameters with minimal performance loss is to use **separable convolution**.\n",
    "\n",
    "The standard convolution has quite a lot of parameters (but still much less than a Fully Connected layer!):\n",
    "\n",
    "![conv](images/conv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(name=\"Conv Model\")\n",
    "conv_model.add(Conv2D(8, kernel_size=(3, 3), use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice: \n",
    "\n",
    "- How many parameters does this convolution has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_model.build((None, *input_shape))\n",
    "# conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separable convolutions are made of two convolutions:\n",
    "\n",
    "- A **depthwise convolution**, a single kernel is created per input channels, spatial information is affected, but channels information is not shared\n",
    "\n",
    "![depthwise conv](images/depthwise.png)\n",
    "\n",
    "- A **pointwise convolution**, is a usual convolution with a kernel of size (1, 1). Spatial information is not affected, but channels information is shared.\n",
    "\n",
    "![pointwise conv](images/pointwise.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import DepthwiseConv2D\n",
    "\n",
    "separable_model = Sequential(name=\"Separable Model\")\n",
    "separable_model.add(DepthwiseConv2D(kernel_size=(3, 3), use_bias=False))\n",
    "separable_model.add(Conv2D(8, kernel_size=(1, 1), use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice:\n",
    "\n",
    "- How many parameters does the Depthwise convolution has?\n",
    "- How many parameters does the Pointwise convolution has?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separable_model.build((None, *input_shape))\n",
    "# separable_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignments\n",
    "\n",
    "- See the different models available on Keras: https://keras.io/applications/ What are their different architecture tricks?\n",
    "- Try to pick an architecture (like [MobileNet](https://arxiv.org/abs/1704.04861) or [Squeeze-and-Excitation network](https://arxiv.org/abs/1709.01507)), read their paper, implement it in Keras, and try to reach good performance on a small dataset like CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
